
---
Author: "Lawrence Koerner"
title: "BagOfWords"
output: html_notebook
---


#Bag of Words
```{r}

string1 <- "Lehigh always beats Laf in the rivalry game because Laf is awful at most things including football. Pathetic!"

string2 <- "Lehighâ€™s football team is known for beating Laf in rivalries, which is not surprising because Lehigh wins mostly everything against that pathetic excuse for a school."

words1 <- strsplit(string1, " ")[[1]]
words2 <- strsplit(string2, " ")[[1]]

words <- unique(c(words1, words2)) #remove duplicates
bagOfWords <- data.frame(row.names = words) #create data frame of words in word1 and word2


for(word in words) {
  if(is.element(word, words1)) {
    bagOfWords[word, "First Sentence"] = 1
  }
  else {
    bagOfWords[word, "First Sentence"] = 0
  }
  
  if(is.element(word, words2)) {
    bagOfWords[word, "Second Sentence"] = 1
  }
  else {
    bagOfWords[word, "Second Sentence"] = 0
  }
}

bagOfWords

```


#Frequent Terms
```{r}
termFreq <- data.frame(row.names = words)
termFreq[, "First Sentence"] = 0
termFreq[, "Second Sentence"] = 0

for (word in words1) {
  if(is.element(word, words1)) { #if word is in words1 increment by 1
    termFreq[word, "First Sentence"] = termFreq[word, "First Sentence"] + 1;
  }
}
for (word in words2) {
  if(is.element(word, words2)) {
    termFreq[word, "Second Sentence"] = termFreq[word, "Second Sentence"] + 1
  }
}

termFreq
```


#Stem Words
```{r}
#install.packages("SnowballC")
library(SnowballC)

#make strings lowercase, remove punctuation, and remove stop words
string1 <- tolower(string1)
string1 <- removePunctuation(string1)
string1 <- removeWords(string1, stopwords("en"))


string2 <- tolower(string2)
string2 <- removePunctuation(string2)
string2 <- removeWords(string2, stopwords("en"))

#remove empty words, and remove stems
newWords1 <- strsplit(string1, " ")[[1]]
newWords1 <- words1[words1 != ""]
newWords2 <- strsplit(string2, " ")[[1]]
newWords2 <- words2[words2 != ""]

#implemented from SnowballC
newWords1 <- stemDocument(words1)
newWords2 <- stemDocument(words2)

newWords1
newWords2

```


#Remove Duplicates
```{r}
newWords <- unique(c(newWords1, newWords2)) #remove duplicates

bagOfWords <- data.frame(row.names = newWords) #create data frame of words in word1 and word2


for(word in newWords) {
  if(is.element(word, newWords1)) {
    bagOfWords[word, "First Sentence"] = 1
  }
  else {
    bagOfWords[word, "First Sentence"] = 0
  }
  
  if(is.element(word, newWords2)) {
    bagOfWords[word, "Second Sentence"] = 1
  }
  else {
    bagOfWords[word, "Second Sentence"] = 0
  }
}

bagOfWords

```



```{r}

termFreq <- data.frame(row.names = newWords)
termFreq[, "First Sentence"] = 0
termFreq[, "Second Sentence"] = 0

for (word in newWords1) {
  if(is.element(word, newWords1)) { #if word is in words1 increment by 1
    termFreq[word, "First Sentence"] = termFreq[word, "First Sentence"] + 1;
  }
}
for (word in newWords2) {
  if(is.element(word, newWords2)) {
    termFreq[word, "Second Sentence"] = termFreq[word, "Second Sentence"] + 1
  }
}

termFreq
```

From stemming the words we see that some words show up more frequently within our termFreq. By removing the stop words, our bagOfWords becomes less cluttered, showing only the important words within the sentences.

Preprocessing data is helpful for search engines to search for specific words while ignoring these stop words. By stemming, search engines can also expand to search for stem names, allowing for these specific words to show up more frequently within an individual search, making it easier for the user to find what they are looking for.
